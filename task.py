"""\--------------------------------------------------------------------------------    ENV: python 2.x    USE: python <PROGNAME> (options)    MAJOR OPTIONS:        -h : print this help message        -n : return the top n; default is 5    SPECIAL OPTIONS:        -t : using tf-idf; default is True        -s : stop_list filename; default is 'stop_list.txt'        -d : the source document filename; default is 'documents.txt'        -i : the filename which stores index; default is 'index.txt'        -q : the query statements filename; default is 'quires.txt'        -o : final result output filename; default is 'output.txt'--------------------------------------------------------------------------------"""from read_documents import ReadDocuments, ReadQueriesfrom nltk.stem import PorterStemmerfrom nltk import word_tokenizeimport os.pathimport jsonimport mathimport sysimport getoptDOC_SIZE = 3204is_tfidf = Trueis_using_stop_list = Truestop_list_filename = 'stop_list.txt'index_filename = 'index.txt'doc_name = 'documents.txt'query_filename = 'queries.txt'result_filename = 'output.txt'stemmer = PorterStemmer()index = {}TOP_N = 5class CommandLine:    def __init__(self):        global is_tfidf, is_using_stop_list, stop_list_filename, index_filename, doc_name, query_filename, result_filename, TOP_N        opts, args = getopt.getopt(sys.argv[1:],'hn:ts:d:i:q:o:')        opts = dict(opts)        if '-h' in opts:            self.printHelp()        if '-n' in opts:            TOP_N = int(opts['-n'])        is_tfidf = '-t' in opts        if '-s' in opts:            stop_list_filename = str((opts['-s']))            is_using_stop_list = True        if '-d' in opts:            doc_name = str((opts['-d']))        if '-i' in opts:            index_filename = str((opts['-i']))        if '-q' in opts:            query_filename = str((opts['-q']))        if '-o' in opts:            result_filename = str((opts['-o']))    def printHelp(self):        help = __doc__.replace('<PROGNAME>',sys.argv[0],1)        print help        sys.exit()class Result:    def __init__(self):        self.weight = 0.0        self.index = -1    def __lt__(self, other):        return self.weight > other.weightdef start_task():    if not os.path.isfile(index_filename):        print '----------computing index---------'        load_docs(doc_name)    print '--------loading index---------'    load_index(index_filename)    print '-----------quering------------'    retrieval_queries(query_filename, result_filename)    print '------------over--------------'def load_index(index_filename):    global index    index = read_index_from_file(index_filename)def load_docs(doc_name):    global DOC_SIZE    documents = ReadDocuments(doc_name)    doc_number = 0    for doc in documents:        all_content = ' '.join(doc.lines)        terms_list = parse_content(all_content)        # build index        for term in terms_list:            doc_count = index.get(term, {})            doc_count[doc.docid] = doc_count.get(doc.docid, 0) + 1            index[term] = doc_count        doc_number += 1    DOC_SIZE = doc_number    store_index_into_file(index_filename)def parse_content(content):    # tokenization    terms_list = tokenization_docs(content)    # remove stop_list    terms_list = removed_stop_list(terms_list, stop_list_filename)    # stemming    terms_list = [stemmer.stem(wd) for wd in terms_list]    return terms_listdef tokenization_docs(content):    terms_list = word_tokenize(content)    return terms_listdef removed_stop_list(terms, filename):    s = set([',', '.', '?', '\'', '"', ':', ';', '(', ')', '!', '`', '&'])    if is_using_stop_list:        with open(filename) as f:            lines = f.readlines()            stop_list = set([line.strip() for line in lines])            s = s.union(stop_list)    ret = []    for term in terms:        if term not in s:            ret.append(term)    return retdef store_index_into_file(index_filename):    f = open(index_filename, 'w')    f.write(json.dumps(index))    f.close()def read_index_from_file(index_filename):    with open(index_filename) as f:        line = f.readline()        index_from_file = json.loads(line)    return index_from_filedef boolean_query(terms):    query_result = []    term_docid = []    for term in terms:        keys = index.get(term, {}).keys()        term_docid.append([int(x) for x in keys])    for i in range(1, DOC_SIZE+1):        r = Result()        r.index = i        for li in term_docid:            if i in li:                r.weight += 1        query_result.append(r)    query_result.sort()    return query_result[:TOP_N]def cal_term_idf_and_doc_size():    global index    term_idf = {}    term_df = {}    doc_size = [0.0 for i in range(DOC_SIZE+1)]    for key, value in index.items():        term_df[key] = len(value.keys())        term_idf[key] =  math.log10(float(DOC_SIZE) / term_df[key])        for doc, count in value.items():            doc_size[int(doc)] += count * count    doc_size = [math.sqrt(x) for x in doc_size]    return term_idf, term_df, doc_sizedef tfidf_query(term_dict):    term_idf, term_df, doc_size = cal_term_idf_and_doc_size()    query_result = []    for i in range(1, DOC_SIZE+1):        r = Result()        r.index = i        qd = 0.0        for key, value in term_dict.items():            docs = index.get(key, {})            tf = docs.get(str(i), 0.0)            idf = term_idf.get(key, 1.0)            qd += tf * idf * value        r.weight = math.sqrt(qd)        query_result.append(r)    query_result.sort()    return query_result[:TOP_N]def retrieval_queries(query_filename, result_filename):    queries = ReadQueries(query_filename)    result = []    for query in queries:        term_list = parse_content(query.content)        if is_tfidf:            term_dict = {}            for term in term_list:                term_dict[term] = term_dict.get(term, 0) + 1            docs = tfidf_query(term_dict)        else:            docs = boolean_query(term_list)        for doc in docs:            result.append(str(query.queid)+' '+str(doc.index))    print '---------writing results---------'    write_result_into_file(result_filename, result)def write_result_into_file(result_filename, data):    output = open(result_filename, 'w')    for item in data:        output.write("%s\n" % item)if __name__ == '__main__':    config = CommandLine()    # start_task()    print is_tfidf, is_using_stop_list, stop_list_filename, index_filename, doc_name, query_filename, result_filename, TOP_N