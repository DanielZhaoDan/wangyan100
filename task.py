from read_documents import ReadDocuments, ReadQueriesfrom nltk.stem import PorterStemmerfrom nltk import word_tokenizeimport os.pathimport jsonimport mathDOC_SIZE = 3204is_tfidf = Trueis_using_stop_list = Truestop_list_filename = 'stop_list.txt'index_filename = 'index.txt'doc_name = 'documents.txt'query_filename = 'queries.txt'stemmer = PorterStemmer()index = {}class Result:    def __init__(self):        self.weight = 0.0        self.index = -1    def __lt__(self, other):        return self.weight > other.weightdef start_task():    if not os.path.isfile(index_filename):        load_docs(doc_name)    load_index(index_filename)    retrieval_queries(query_filename)def load_index(index_filename):    global index    index = read_index_from_file(index_filename)def load_docs(doc_name):    global DOC_SIZE    documents = ReadDocuments(doc_name)    doc_number = 0    for doc in documents:        all_content = ' '.join(doc.lines)        terms_list = parse_content(all_content)        # build index        for term in terms_list:            doc_count = index.get(term, {})            doc_count[doc.docid] = doc_count.get(doc.docid, 0) + 1            index[term] = doc_count        doc_number += 1    DOC_SIZE = doc_number    store_index_into_file(index_filename)def parse_content(content):    # tokenization    terms_list = tokenization_docs(content)    # remove stop_list    terms_list = removed_stop_list(terms_list, stop_list_filename)    # stemming    terms_list = [stemmer.stem(wd) for wd in terms_list]    return terms_listdef tokenization_docs(content):    terms_list = word_tokenize(content)    return terms_listdef removed_stop_list(terms, filename):    s = set([',', '.', '?', '\'', '"', ':', ';', '(', ')', '!', '`', '&'])    if is_using_stop_list:        with open(filename) as f:            lines = f.readlines()            stop_list = set([line.strip() for line in lines])            s = s.union(stop_list)    ret = []    for term in terms:        if term not in s:            ret.append(term)    return retdef store_index_into_file(index_filename):    f = open(index_filename, 'w')    f.write(json.dumps(index))    f.close()def read_index_from_file(index_filename):    with open(index_filename) as f:        line = f.readline()        index_from_file = json.loads(line)    return index_from_filedef boolean_query(terms):    query_result = []    term_docid = []    for term in terms:        keys = index.get(term, {}).keys()        term_docid.append([int(x) for x in keys])    for i in range(1, DOC_SIZE+1):        r = Result()        r.index = i        for li in term_docid:            if i in li:                r.weight += 1        query_result.append(r)    query_result.sort()    return query_resultdef cal_term_tfidf_and_doc_size():    global index    term_tfidf = {}    term_df = {}    doc_size = [0.0 for i in range(DOC_SIZE+1)]    for key, value in index.items():        term_df[key] = len(value.keys())        term_tfidf[key] = math.log10(float(DOC_SIZE) / term_df[key])        for doc, count in value.items():            doc_size[int(doc)] += count * count    doc_size = [math.sqrt(x) for x in doc_size]    return term_tfidf, term_df, doc_sizedef tfidf_query(term_dict):    term_tfidf, term_df, doc_size = cal_term_tfidf_and_doc_size()    query_result = []    for i in range(1, DOC_SIZE+1):        r = Result()        r.index = i        qd = 0.0        for key, value in term_dict.items():            docs = index.get(key, {})            qd += value * docs.get(str(i), 0.0)        r.weight = math.sqrt(qd) / doc_size[i]        query_result.append(r)    query_result.sort()    return query_resultdef retrieval_queries(query_filename):    queries = ReadQueries(query_filename)    for query in queries:        term_list = parse_content(query.content)        if is_tfidf:            term_dict = {}            for term in term_list:                term_dict[term] = term_dict.get(term, 0) + 1            result = tfidf_query(term_dict)        else:            result = boolean_query(term_list)        print(result[0].weight)        for res in result:            if res.index == 1410 or res.index == 1572 or res.index == 1605 or res.index == 2020:                print res.index, res.weight        breakif __name__ == '__main__':    start_task()